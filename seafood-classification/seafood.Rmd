---
title: "seafood"
author: "Alan Nurcahyo"
date: "9/5/2020"
output: pdf_document
---

```{r message=FALSE}
#library used
library(tidyverse)
library(caret)
library(quanteda)
library(doSNOW) #run model to run in each core
library(googleLanguageR)
library(irlba)
library(class)
```

# Preprocessing
## Load and review dataset
```{r}
#load dataset
df <- read_csv("/Users/alannurcahyo/Documents/Documents/AU/fall2020/DATA/Seafood_Dishes.csv")
##check label
unique(df$seafood_yn) 

# #create labeled dataset
# df.labeled <- df %>%
#   filter(seafood_yn %in% c(0,1))
# prop.table(table(df.labeled$seafood_yn))
# 
# #create unlabeled dataset
# df.unlabeled <- df %>%
#   filter(!seafood_yn %in% c(0,1))

df.labeled <- read_csv("~/Downloads/dfnew_labeled_20201012.csv")

df.unlabeled <- df %>%
  anti_join(df.labeled, by = "id")

## check
nrow(df) == nrow(df.labeled) + nrow(df.unlabeled)
unique(df.unlabeled$seafood_yn)
```
Evaluate distribution of labeled and non labeled by Year
```{r}
#Histogram of first time the menu appeared
# It appears that the labeled data lack of observation from later year
df %>%
  filter(first_appeared != 0,
         first_appeared != 1,
         first_appeared != 2928) %>%
  mutate( seafoonTF = !is.na(seafood_yn)) %>%
  ggplot(aes(first_appeared, fill = seafoonTF)) + 
  geom_histogram(binwidth = 10)

#Histogram of last time the menu appeared
# It appears that the labeled data has a sufficient range of observation
df %>%
  filter(last_appeared != 0,
         last_appeared != 1,
         last_appeared != 2928) %>%
  mutate( seafoonTF = !is.na(seafood_yn)) %>%
  ggplot(aes(last_appeared, fill = seafoonTF)) + 
  geom_histogram(binwidth = 10)

# comparison between labeled and unlabeled data by first appeared
# It appears that labeled data lack of observation after 1908 
count.first.appeared <- df %>%
  filter(first_appeared != 0,
         first_appeared != 1,
         first_appeared != 2928) %>%
  mutate( seafoodTF = !is.na(seafood_yn)) %>%
  group_by(first_appeared, seafoodTF) %>%
  count() %>%
  spread(seafoodTF, n)

# number of unlabeled observation after 1098 
sum(count.first.appeared$`FALSE`[count.first.appeared$first_appeared>1907])

# number of labeled observation after 1098 
sum(count.first.appeared$`TRUE`[count.first.appeared$first_appeared>1907], na.rm = TRUE)

# let's take one percent of oberservation after 1908 to be labeled and round it
# per year we need following observation
n_new_sample <- count.first.appeared %>%
  mutate(n_sample = round(`FALSE`/100,0)) %>%
  select(first_appeared,n_sample) %>%
  filter(first_appeared >1907,
         n_sample >0)

# ## we take n number of observation from unlabeled dataset by following the number we need on n_new_sample$n_sample
# new_sample <- vector(mode = "list", length = nrow(n_new_sample))
# for (i in seq_along(new_sample)) {
#   new_sample[[i]] <- df.unlabeled %>%
#     filter(first_appeared == n_new_sample$first_appeared[[i]]) %>%
#     sample_n(n_new_sample$n_sample[[i]])
# }
# new_sample <- do.call(rbind, new_sample)
# 
# # write it to csv
# # write_csv(new_sample,"new_sample.csv")
# # label it manually, then load the labeled new sample into R
# new_sample_labeled <- read_csv("new_sample_labeled.csv")
# new_sample_labeled <- new_sample_labeled %>%
#   filter(seafood_yn %in% c(0,1))
# head(new_sample_labeled)
```

Translate
```{r}
###################### use google API #########################
## the output file is saved in .csv because it is costly and time consuming to run it everytime
# gl_auth("seafood-translation-f329aa685c7f.json")
# df.name.translate <- vector(mode = "list", length = nrow(df.labeled))
# 
# for (i in seq_along(df.name.translate)) {
#      df.name.translate[[i]] <- gl_translate(df.labeled$name[i], target = "en")
# }
# 
# df.name.translate.bind <- do.call(rbind, df.name.translate)
# prop.table(table(df.name.translate.bind$detectedSourceLanguage))
# 
# write.csv(df.name.translate.bind,"NameTranslated.csv")
df.name.translate.bind <- read_csv("NameTranslated.csv")
# 
# # translate new_sample_labeled
# gl_auth("seafood-translation-2ae5d5055080.json")
# df.name.translate.newsample <- vector(mode = "list", length = nrow(new_sample_labeled))
# #
# for (i in seq_along(df.name.translate.newsample)) {
#       df.name.translate.newsample[[i]] <- gl_translate(new_sample_labeled$name[i], target = "en")
# }
# 
#  df.name.translate.newsample <- do.call(rbind, df.name.translate.newsample)
df.name.translate.newsample <- read_csv("new_sample_translated.csv")

df.labeled.translate <- rbind(df.name.translate.bind[,-1], df.name.translate.newsample)

##check wheter we can cbind translated dish name and label from labeled data set
check<- df.labeled$name == df.labeled.translate$text
check[FALSE]

# translate unlabeled
# gl_auth("seafood-translation-2ae5d5055080.json")
# df.unlabeled.translate <- vector(mode = "list", length = nrow(df.unlabeled))
# #
# for (i in 300001:392095) {
#       df.unlabeled.translate[[i]] <- gl_translate(df.unlabeled$name[i], target = "en")
#       print(i)
# }
# 
# 
#  df.unlabeled.translate.part4 <- do.call(rbind, df.unlabeled.translate[300001:392095])
#  write_csv(df.unlabeled.translate.part4, file = "unlabeledtranslate300001toend.csv")
#  
#  rm(df.unlabeled.translate.part3)

### load translated documents for unlabeled
part1 <- read_csv("unlabeledtranslate1to100000.csv")
part2 <- read_csv("unlabeledtranslate100001to200000.csv")
part3 <- read_csv("unlabeledtranslate200001to300000.csv")
part4 <- read_csv("unlabeledtranslate300001toend.csv")

unlabeled.translated <- rbind(part1[,-1],part2[,-1],part3[,-1],part4)
nrow(unlabeled.translated) == nrow(df.unlabeled)

## check distribution
#Histogram of first time the menu appeared
df.labeled %>%
  filter(first_appeared != 0,
         first_appeared != 1,
         first_appeared != 2928) %>%
  mutate( seafoonTF = !is.na(seafood_yn)) %>%
  ggplot(aes(first_appeared, fill = seafoonTF)) + 
  geom_histogram(binwidth = 10)

# comparison between labeled and unlabeled data by first appeared
count.first.appeared <- df.labeled %>%
  filter(first_appeared != 0,
         first_appeared != 1,
         first_appeared != 2928) %>%
  mutate( seafoodTF = !is.na(seafood_yn)) %>%
  group_by(first_appeared, seafoodTF) %>%
  count() %>%
  spread(seafoodTF, n)

## add more label for pattern that are not available in training (see steps on appendix)
## our pattern
new.labeled.22oct <- df.unlabeled %>%
  filter(id %in% sampleoct22.vector)
write.csv(new.labeled.22oct, "newlabeled22oct.csv")
```
## managing dataset after translation and new added label
```{r}
## check
(df.unlabeled$name == unlabeled.translated$text)[FALSE]
#merge
df.unlabeled <- cbind(df.unlabeled, translatedText = unlabeled.translated$translatedText)

## check
(df.labeled$name == df.labeled.translate$text)[FALSE]
## merge
df.labeled <- cbind(df.labeled, translatedText = df.labeled.translate$translatedText)

## append them all
df.all <- rbind(df.unlabeled, df.labeled)
## check
nrow(df.all) == nrow(df)
```


Adding, mutating variable
```{r}
df.labeled <- data.frame(name = df.labeled$name,
                        translatedText = df.labeled.translate$translatedText, 
                         seafood_yn =df.labeled$seafood_yn)
### adding new variable to df.labeled to improve model performance
popularseafoodwords <- c("oyster", "Oyster", "crab","Crab", "clam", "Clam","lobster","Lobster", 
                         "salmon", "Salmon", "bass", "Bass", "mackerel", "Mackerel","halibut","Halibut")
popularseafoodwords <- paste(popularseafoodwords,collapse = "|")
df.labeled <- df.labeled %>%
  mutate(popseafood = ifelse(str_detect(translatedText,popularseafoodwords),1,0),
         digit4 = ifelse(str_detect(translatedText,"[:digit:]{4}"),1,0))
head(df.labeled)
```

# create training and test

```{r}
set.seed(1234)

#create index of stratified 80:20 split that maintain proportion of label.
index <- caret::createDataPartition(df.labeled$seafood_yn, times = 1,
                                    p = 0.8, list= FALSE) 

# train and test
train <- df.labeled[index,]
test <- df.labeled[-index,]

##verify
prop.table(table(train$seafood_yn))
prop.table(table(test$seafood_yn))

save(train, file = "train.Rdata")
save(test, file = "test.Rdata")
```


## preprocess training set
### preprocess training set using only noun as data frame
```{r}
## annotate dish name

library(udpipe)
dl <- udpipe_download_model(language = "english")
udmodel<- udpipe_load_model(file = dl$file_model)
annotate <- udpipe_annotate(udmodel, train$translatedText)
annotate <- as.data.frame(annotate)
table(annotate$upos)
Noun <- annotate %>%
  filter(upos == "NOUN") %>%
  dplyr::select(token)
Noun <- Noun[1:nrow(Noun),]
Noun <- unique(Noun)

#create a corpus
corpus <- corpus(train$translatedText) 

#create document frequency matrix and clean the corpus
dfm.train <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)

# take only Noun
dfm.train.Noun <- dfm_select(dfm.train, pattern = Noun)
dim(dfm.train.Noun)

##setup feature dataframe with label
train.dfm.allword <- data.frame(label=as.factor(train$seafood_yn),
                        digit4 = train$digit4,
                        dfm.train) %>%
  select(-doc_id)

##setup feature dataframe with label
train.dfm <- data.frame(label=as.factor(train$seafood_yn),
                        digit4 = train$digit4,
                        dfm.train.Noun) %>%
  select(-doc_id)
head(train.dfm)
save(train.dfm, file = "traindfm.Rdata")
```
### preprocess training set using all words then convert it to 300 variable using svd
```{r}

### we start our process from dfm.train (a table that still have all corpus)
dim(dfm.train)

############## make TF-IDF version of data ##################
## make tf function
term.frequency <- function(row){
  row/sum(row)
}

## make function to calculate IDF
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col>0))
  log10(corpus.size/doc.count)
}

## make function to calculate tf/idf

tf.idf <- function(tf,idf) {
  tf*idf
}

## make tf matrix transformation 
dfm.train.tf <- apply(dfm.train, 1, term.frequency)
dim(dfm.train)

## make idf . NOTe: This IDF will be used again later in test dataset
dfm.train.idf <- apply(dfm.train, 2, inverse.doc.freq)

## make tf-idf matrix 
dfm.train.tfidf <- apply(dfm.train.tf, 2, tf.idf, idf = dfm.train.idf)
dim(dfm.train.tfidf)

## transpose t-idf matrix
dfm.train.tfidf <- t(dfm.train.tfidf)
dim(dfm.train.tfidf)

## check incomplete cases -- it is possible that with our preprocessing some dish 
## have name that being removed
incomplete.cases <- which(!complete.cases(dfm.train.tfidf))
train$translatedText[incomplete.cases]

## fix it
dfm.train.tfidf[incomplete.cases,] <- rep(0.0, ncol(dfm.train.tfidf))

################ end of tf-idf ##################


########## Singular value decomposition ##########

## record start time
start.time <- Sys.time()

### perform SVD to decrease dimentionality  to 300 using package irlba
train.irlba <- irlba(t(dfm.train.tfidf), nv = 300, maxit = 600)

## total time ~12 minutes
total.time <- Sys.time() - start.time 


## save svd value for test data
sigma.inverse <- 1/train.irlba$d
u.transpose <- t(train.irlba$u)

##setup feature dataframe with label

train.svd <- data.frame(label=as.factor(train$seafood_yn), 
                        popseafood = train$popseafood, 
                        digit4 = train$digit4,
                        train.irlba$v)
dim(train.svd)

save(train.svd,file = "trainingsvd.Rdata")
##load("trainingsvd.Rdata")
head(train.svd)


#################### svd part II ##########3
## we make another dataset with singular value decomposition 
## however, this time we did not weighting data based on tf-idf function

## record start time
start.time <- Sys.time()

### perform SVD to decrease dimentionality  to 300 using package irlba
train.irlba.2 <- irlba(t(dfm.train), nv = 300, maxit = 600)

## total time ~12 minutes
total.time <- Sys.time() - start.time 


## save svd value for test data
sigma.inverse.2 <- 1/train.irlba.2$d
u.transpose.2 <- t(train.irlba.2$u)
##setup feature dataframe with label

train.svd.2 <- data.frame(label=as.factor(train$seafood_yn), 
                        popseafood = train$popseafood, 
                        digit4 = train$digit4,
                        train.irlba.2$v)
dim(train.svd)

save(train.svd.2,file = "trainingsvd2.Rdata")
##load("trainingsvd.Rdata")
head(train.svd.2)

################# end of SVD ###################################
```

## preprocessing test
### preprocess test set using only noun as data frame
```{r}

############## choosing only Noun ##############
#create a corpus
corpus <- corpus(test$translatedText) 

#create document frequency matrix with verb
dfm.test <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)


dfm.test.allword <- dfm_select(dfm.test, pattern = dfm.train)


dfm.test.Noun <- dfm_select(dfm.test, pattern = dfm.train.Noun)
dim(dfm.train.Noun)

############### setup feature dataframe with label   #####################
##setup feature dataframe with label
test.dfm.allword <- data.frame(label=as.factor(train$seafood_yn),
                        digit4 = train$digit4,
                        dfm.test.allword) %>%
  select(-doc_id)
dim(test.dfm.allword)

test.dfm <- data.frame(label=as.factor(test$seafood_yn),
                       digit4 = test$digit4, 
                       dfm.test.Noun) %>%
  select(-doc_id)

save(test.dfm, file = "testdfm.Rdata")
```

### preprocess test set using all words then convert it to 300 variable using svd
```{r}
#we already have document frequancy matrix above

### convert test data set to have the samee dimension as training data ##
dfm.test <- dfm_select(dfm.test, pattern = dfm.train)
dfm.test.matrix <- as.matrix(dfm.test)
dim(dfm.test)

############## make TF-IDF version of data ##################

## make tf matrix transformation 
dfm.test.tf <- apply(dfm.test.matrix, 1, term.frequency)


## make tf-idf matrix using idf from training data, 
## because we expect that any new document that comes in the future has to
## correspondent with our main corpus (training data)
dfm.test.tfidf <- apply(dfm.test.tf, 2, tf.idf, idf = dfm.train.idf)
dim(dfm.test.tfidf)

## transpose t-idf matrix
dfm.test.tfidf <- t(dfm.test.tfidf)
dim(dfm.test.tfidf)

## check incomplete cases -- it is possible that with our preprocessing some dish 
## have name that being removed
incomplete.cases <- which(!complete.cases(dfm.test.tfidf))

## fix it
dfm.test.tfidf[incomplete.cases,] <- rep(0.0, ncol(dfm.test.tfidf))

################ end of tf-idf ##################


########## Singular value decomposition ##########

test.svd <- t(sigma.inverse*u.transpose %*% t(dfm.test.tfidf))

############### setup feature dataframe with label   #####################
test.svd <- data.frame(label=as.factor(test$seafood_yn), 
                       popseafood = test$popseafood, 
                       digit4 = test$digit4, 
                       test.svd)
head(test.svd)

save(test.svd,file = "testsvd.Rdata")
load("testsvd.Rdata")

########## Singular value decomposition without wighting ##########

test.svd.2 <- t(sigma.inverse.2*u.transpose.2 %*% t(as.matrix(dfm.test)))

############### setup feature dataframe with label   ##########
test.svd.2 <- data.frame(label=as.factor(test$seafood_yn), 
                       popseafood = test$popseafood, 
                       digit4 = test$digit4, 
                       test.svd.2)
dim(test.svd.2)

save(test.svd.2,file = "testsvd2.Rdata")
load("testsvd2.Rdata")

head(test.svd.2)

################# end of SVD ###################################
```

## model performance evaluation
```{r}
## create a function to determine best model
## we determine best model based on accuracy
## the fuction will return the best accuracy, tuning with the best accuracy,
## and confusion matrix for the model with the best accuracy
best.accuracy <- function(result, testdf) {
  accuracy <- 1:length(result)
  n.test <- nrow(testdf)
  for (i in seq_along(result)) {
    accuracy[[i]] <- sum(testdf$label == result[[i]] )/n.test 
  }
  CPR <- max(accuracy)
  k <- which.max(accuracy)
  conf.matrix <- confusionMatrix(as.factor(result[[k]]),testdf$label)
  best.model <- list(max_correct_predicted_rate =CPR,
                     best_thershold = k,
                     best_confusion.matric = conf.matrix)
  return(best.model)
}
```



## use KNN

```{r}
######### knn #########################

# KNN is a non-parameteric classification method where in order to predict a response class
# we use the value of k-nearest neighbour in the data set. k indicate the number of neighbour we want to 
# use in the model. The distance between neighbour is determined using Euclidean Distance


################# use 10-fold cv ######################
# use caret package to make stratified 10-fold cross validation repeated
# # create index to 10-fold cv, use ROC as a model selection criteria
# cv.control <- trainControl(method = "cv", number = 10, 
#                            summaryFunction = twoClassSummary,
#                            classProbs = TRUE)
# 
# # time the code execution
# start.time <- Sys.time()
# 
# # knnFit <- train(label ~ ., data = train.svd, 
# #                 method = "knn", 
# #                 trControl = cv.control, 
# #                 preProcess = c("center","scale"), 
# #                 tuneLength = 10)
# 
# total.time <- Sys.time() - start.time
## Took about 3 hours to train the model
# ## save(knnFit,file = "knnFit.Rdata")
# load("knnFit.Rdata")
# knnFit
# plot(knnFit)

##################### using simple test and training ##################

################ using dfm version of data ###########

# we can not use this type of data using KNN, because the alue between observation is 
# too much similar, there will be too many ties


########### ends of model using dfm version of data ##################


################ using svd version of data ###########
knn.result.svd <- vector(mode = "list",length=3)
tune.knn <- c(5,25,50,100,150) #low k might be overfit models

set.seed(1234)
for (i in seq_along(tune.knn)) {
  knn.result.svd[[i]] <- knn(train.svd[,-1],test.svd[,-1],train.svd$label, k = tune.knn[i])
}
save(knn.result.svd, file = "knn.result.svd.Rdata")
best.accuracy(knn.result.svd,testdf = test.svd)
system("say just finished")

test.predicted.actual.knnsvd <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = knn.result.svd[[1]]) ## best threshold is k = 5
test.predicted.actual.knnsvd %>%
  filter(actual != predicted)


################ using svd without weighting of data ###########
knn.result.svd.2 <- vector(mode = "list",length=3)
tune.knn <- c(5,25,50,100,150) #low k might be overfit models

set.seed(1234)
for (i in seq_along(tune.knn)) {
  knn.result.svd.2[[i]] <- knn(train.svd.2[,-1],test.svd.2[,-1],train.svd.2$label, k = tune.knn[i])
}
save(knn.result.svd.2, file = "knnresultsvd2.Rdata")
best.accuracy(knn.result.svd.2,testdf = test.svd.2)
system("say just finished")

test.predicted.actual.knnsvd.2 <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = knn.result.svd.2[[1]]) ## best threshold is k = 5
test.predicted.actual.knnsvd.2 %>%
  filter(actual != predicted)
########### ends of model using svd version of data ##################
########### ends of model using svd version of data ##################
```


## use logistic regression

```{r}
#using simple test and training, we previously use cross validation but ends up too long
############# using svd version of data #########
# time the code execution
start.time <- Sys.time()

logfit.svd <- glm(label ~ ., data = train.svd, family=binomial)

total.time.train.log.svd <- Sys.time() - start.time

## Took less than a minute to return a model
total.time.train.log.svd

##################### predict with test ##################
# predict
logpredict <- predict(logfit.svd, newdata = test.svd,  type = "response")
log.result.svd <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  log.result.svd[[i]] <- 1*(logpredict > i/100 )
}

## determine best model
best.accuracy(log.result.svd, testdf = test.svd)
system("say just finished")


### examine the result 
library(broom)
coef.table.log.svd <- tidy(logfit.svd)
coef.table.log.svd
test.predicted.actual.logsvd <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = log.result.svd[[46]]) ## best threshold is 0.01
test.predicted.actual.logsvd %>%
  filter(actual != predicted)


############# using svd without weighting of data #########
# time the code execution
start.time <- Sys.time()

logfit.svd.2 <- glm(label ~ ., data = train.svd.2, family=binomial)

total.time.train.log.svd.2 <- Sys.time() - start.time

## Took less than a minute to return a model
total.time.train.log.svd.2

##################### predict with test ##################
# predict
logpredict.2 <- predict(logfit.svd.2, newdata = test.svd.2,  type = "response")
log.result.svd.2 <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  log.result.svd.2[[i]] <- 1*(logpredict.2 > i/100 )
}

## determine best model
best.accuracy(log.result.svd.2, testdf = test.svd.2)
system("say just finished")


### examine the result 
library(broom)
coef.table.log.svd.2 <- tidy(logfit.svd.2)
coef.table.log.svd.2
test.predicted.actual.logsvd.2 <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = log.result.svd.2[[29]]) ## best threshold is 0.29
actualvpredicted <- test.predicted.actual.logsvd.2 %>%
  filter(actual != predicted)
write_csv(actualvpredicted, "actualvpredicted_misclassified.csv")


############# using dfm with only Noun version of data #########
# time the code execution


logfit.noun <- glm(label ~ ., data = train.dfm, family=binomial)
## there are multicollinerity problem in dataset
coef.table.log.noun <- tidy(logfit.noun)
coef.sig <- coef.table.log.noun %>%
  filter(p.value < 0.1)
term.sig <- paste(coef.sig$term[-1] ,collapse = "+")
logfit.noun <- glm(paste("label~", term.sig, sep = ""), data = train.dfm, family=binomial)

## Took less than a minute to return a model
# total.time.train.log.noun

##################### predict with test ##################
# predict
logpredict <- predict(logfit.noun, newdata = test.dfm,  type = "response")

log.result.noun <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  log.result.noun[[i]] <- 1*(logpredict > i/100 )
}

## determine best model
best.accuracy(log.result.noun, testdf = test.dfm)
system("say just finished")


test.predicted.actual.lognoun <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         prob = logpredict,
                         predicted = log.result.noun[[34]]) ## best threshold is 0.27 #
test.predicted.actual.lognoun %>%
  filter(actual != predicted)

############# using dfm with all words available to train of data #########
# time the code execution
start.time <- Sys.time()

logfit.allword <- glm(label ~ ., data = train.dfm.allword, family=binomial)
coef.table.log.allword <- tidy(logfit.allword)
coef.sig <- coef.table.log.allword %>%
  filter(p.value < 0.1)
term.sig <- paste(coef.sig$term[-1] ,collapse = "+")
logfit.allword <- glm(paste("label~", term.sig, sep = ""), data = train.dfm.allword, family=binomial)

total.time.train.log.allword <- Sys.time() - start.time

## Took less than a minute to return a model
total.time.train.log.allword

##################### predict with test ##################
# predict
logpredict <- predict(logfit.allword, newdata = test.dfm.allword,  type = "response")
log.result.allword <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  log.result.allword[[i]] <- 1*(logpredict > i/100 )
}

## determine best model
best.accuracy(log.result.allword, testdf = test.dfm.allword)
system("say just finished")

coef.table.log.allword <- tidy(logfit.allword)
coef.table.log.allword %>%
  arrange(desc(estimate))
test.predicted.actual.logallword <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = log.result.allword[[27]]) ## best threshold is 0.33
test.predicted.actual.logallword %>%
  filter(actual != predicted)
```
## APPENDIX TO DO
## unique courpus(((df.labeled$dish) vs unique(dfulabeled$dish)
## take dish that has logit close to trhreshold
## make .csv for predicted != actual to check - done
## quick summary on where we are right now 

```{r}
## take dish that has logit close to trhreshold
####### logfit.noun ######
index.range <- logpredict > 0.3 & logpredict < 0.38
test.predicted.actual.lognoun[index.range,]%>%
  filter(actual != predicted)

#svd unweighted
index.range.2 <- logpredict.2 > 0.20 & logpredict.2 < 0.40 ##threshold = 0.29
test.predicted.actual.logsvd.2[index.range.2,]%>%
  filter(actual != predicted)

#create a corpus
corpus <- corpus(unlabeled.translated$translatedText) 

#create document frequency matrix with verb
dfm.unlabeled <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)

count.train <- apply(dfm.train, 2, sum)
count.test <- apply(dfm.test,2,sum)
## too big to count
# count.unlabeled <- apply(dfm.unlabeled, 2, sum)
# try to trim it
dfm.unlabeled.trimmed <- dfm_trim(dfm.unlabeled, min_docfreq = 15, verbose = TRUE)
dim(dfm.unlabeled.trimmed)
count.unlabeled <- apply(dfm.unlabeled.trimmed[1:100000,], 2, sum)
count.unlabeled2 <- apply(dfm.unlabeled.trimmed[100001:200000,], 2, sum)
count.unlabeled3 <- apply(dfm.unlabeled.trimmed[200001:300000,], 2, sum)
count.unlabeled4 <- apply(dfm.unlabeled.trimmed[300001:392095,], 2, sum)
count.unlabeled <- count.unlabeled+count.unlabeled2+count.unlabeled3+count.unlabeled4

count.train <- data.frame(name = names(count.train),
           freq = count.train)

count.test <- data.frame(name = names(count.test),
           freq = count.test)

count.unlabeled <- data.frame(name = names(count.unlabeled),
           freq = count.unlabeled)

## top words
topterm.train <- count.train %>%
  arrange(desc(freq)) %>%
  slice(1:10000)

topterm.test <- count.test %>%
  arrange(desc(freq)) %>%
  slice(1:1000)

topterm.unlabeled <- count.unlabeled %>%
  arrange(desc(freq)) %>%
  slice(1:3000)

nothing.on.labeled <- topterm.unlabeled %>%
  left_join(topterm.train, by = "name") %>%
  left_join(topterm.test, by = "name") %>%
  filter(is.na(freq.y) & is.na(freq))

nothing.on.labeled.smp <- nothing.on.labeled %>%
  mutate(freq.x = round(freq.x/100)+1)

sampleoct22 <- vector(mode = "list", length = nrow(nothing.on.labeled))
for (i in seq_along(sampleoct22)) {
  sampleoct22[[i]] <- df.unlabeled %>%
    filter(str_detect(translatedText,nothing.on.labeled$name[i])) %>%
    select(id, first_appeared) %>%
    arrange(desc(first_appeared)) %>%
    slice(1:nothing.on.labeled.smp$freq.x[[i]]) %>% ##take number of dish from latest dish (we low on new name dish)
    select(id)
}

sampleoct22 <- do.call(rbind, sampleoct22)
sampleoct22 <- unique(sampleoct22)
sampleoct22.vector <- sampleoct22$id
```






# make models use random forest 
## use R default setting - Error: protect(): protection stack overflow 
## - try to tweak default R setting: Error: vector memory exhausted (limit reached?)
## solution to try: use machine with bigger memory
```{r}
# set.seed(1234)
# 
# cv.folds <- createMultiFolds(train$seafood_yn, k = 10, times = 1)
# cv.control <- trainControl(method = "repeatedcv", number = 10,
#                            repeats = 1, index = cv.folds)
# 
# cl <- DoSNOW::makeCluster(20)
# registerDoSNOW(cl)
# 
# # time the code execution
# start.time <- Sys.time()
# 
# # build decision tree - rpart
# rpart.cv.1 <- train(label~.,data = train.svd, 
#                     method = "rf",
#                     trcontrol = cv.control,
#                     tuneLength = 10)
# 
# total.time <- Sys.time() - start.time
# 
# DoSNOW::stopCluster(cl)
load("trainingrf.Rdata")
rf.cv.1
# rfpredict <- predict(rf.cv.1, newdata = test.svd)
load("rfpredict.rdata")
confusionMatrix(rfpredict, test.svd$label)
```

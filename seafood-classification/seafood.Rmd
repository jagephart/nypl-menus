---
title: "seafood"
author: "Alan Nurcahyo"
date: "9/5/2020"
output: pdf_document
---

```{r message=FALSE}
#library used
library(tidyverse)
library(caret)
library(quanteda)
library(doSNOW) #run model to run in each core
library(googleLanguageR)
library(irlba)
library(class)
```

## Load dataset
```{r}
df.labeled <- read_csv("dflabeledtranslated_edited.csv")
df.labeled <- df.labeled %>%
  select(name, translatedText, seafood_yn)
head(df.labeled)
prop.table(table(df.labeled$seafood_yn))
unique(df.labeled$seafood_yn)
```
Adding, mutating variable
```{r}
### adding new variable to df.labeled to improve model performance
popularseafoodwords <- c("oyster", "Oyster", "crab","Crab", "clam", "Clam","lobster","Lobster", 
                         "salmon", "Salmon", "bass", "Bass", "mackerel", "Mackerel","halibut","Halibut",
                         "seafood", "Seafood","turbot","Turbot", "fish","Fish")
popularseafoodwords <- paste(popularseafoodwords,collapse = "|")
df.labeled <- df.labeled %>%
  mutate(popseafood = ifelse(str_detect(translatedText,popularseafoodwords),1,0),
         digit4 = ifelse(str_detect(translatedText,"[:digit:]{4}"),1,0))
head(df.labeled)

df.labeled %>%
  ggplot(aes(seafood_yn, popseafood)) + geom_jitter() + theme_bw()
df.labeled %>%
  ggplot(aes(seafood_yn, digit4)) + geom_jitter() + theme_bw()

table(df.labeled$seafood_yn, df.labeled$popseafood)
table(df.labeled$seafood_yn, df.labeled$digit4)

df.labeled %>%
  filter(seafood_yn == 0 & popseafood ==1) ##some are potentially wrong label
```

# create training and test

```{r}
###### create training and test with the same prob #########
set.seed(1234)

#create index of stratified 80:20 split that maintain proportion of label.
index <- caret::createDataPartition(df.labeled$seafood_yn, times = 1,
                                    p = 0.8, list= FALSE) 

# train and test
train <- df.labeled[index,]
test <- df.labeled[-index,]

##verify
prop.table(table(train$seafood_yn))
prop.table(table(test$seafood_yn))

save(train, file = "train.Rdata")
save(test, file = "test.Rdata")
load("test.Rdata")
table(train$seafood_yn)

###### create training considering class imbalance with undersampling method #########
library(ROSE)
trainunder <- ovun.sample(seafood_yn~., data = train, method = "under", seed = 1)$data
```


## preprocess training set
### preprocess training set without undersampling
```{r}
## annotate dish name

library(udpipe)
dl <- udpipe_download_model(language = "english")
udmodel<- udpipe_load_model(file = dl$file_model)
annotate <- udpipe_annotate(udmodel, train$translatedText)
annotate <- as.data.frame(annotate)
table(annotate$upos)
Noun <- annotate %>%
  filter(upos == "NOUN") %>%
  dplyr::select(token)
Noun <- Noun[1:nrow(Noun),]
Noun <- unique(Noun)

#create a corpus
corpus <- corpus(train$translatedText) 

#create document frequency matrix and clean the corpus
dfm.train <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)

# take only Noun
dfm.train.Noun <- dfm_select(dfm.train, pattern = Noun)
dim(dfm.train.Noun)

##setup feature dataframe with label
train.dfm.allword <- data.frame(label=as.factor(train$seafood_yn),
                        digit4 = train$digit4,
                        dfm.train) %>%
  select(-doc_id)

##setup feature dataframe with label
train.dfm.noun <- data.frame(label=as.factor(train$seafood_yn),
                        digit4 = train$digit4,
                        dfm.train.Noun) %>%
  select(-doc_id)
head(train.dfm.noun)
save(train.dfm.noun, file = "traindfmnoun.Rdata")
save(train.dfm.allword, file = "traindfmall.Rdata")
  
```

### preprocess training undersampling
```{r}
## annotate dish name
#create a corpus
corpus <- corpus(trainunder$translatedText) 

#create document frequency matrix and clean the corpus
dfm.train.under <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)

##setup feature dataframe with label
train.dfm.under <- data.frame(label=as.factor(trainunder$seafood_yn),
                        digit4 = trainunder$digit4,
                        dfm.train.under) %>%
  select(-doc_id)
save(train.dfm.under, file = "traindfmunder.Rdata")
load("traindfmunder.Rdata")
```

### preprocess training set using all words then convert it to 300 variable using svd
```{r}

### we start our process from dfm.train (a table that still have all corpus)
dim(dfm.train)

########## Singular value decomposition ##########

## record start time
start.time <- Sys.time()

### perform SVD to decrease dimentionality  to 300 using package irlba
train.irlba.2 <- irlba(t(dfm.train), nv = 300, maxit = 600)

## total time ~12 minutes
total.time <- Sys.time() - start.time 


## save svd value for test data
sigma.inverse.2 <- 1/train.irlba.2$d
u.transpose.2 <- t(train.irlba.2$u)
##setup feature dataframe with label

train.svd <- data.frame(label=as.factor(train$seafood_yn), 
                        popseafood = train$popseafood, 
                        digit4 = train$digit4,
                        train.irlba.2$v)

save(train.svd,file = "trainingsvd.Rdata")
load("trainingsvd.Rdata")
head(train.svd)

################# end of SVD ###################################
```

## preprocessing test
### preprocess test set
```{r}

############## choosing only Noun ##############
#create a corpus
corpus <- corpus(test$translatedText) 

#create document frequency matrix with verb
dfm.test <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)


dfm.test.allword <- dfm_select(dfm.test, pattern = dfm.train)


dfm.test.Noun <- dfm_select(dfm.test, pattern = dfm.train.Noun)

############### setup feature dataframe with label   #####################
##setup feature dataframe with label
test.dfm.allword <- data.frame(label=as.factor(test$seafood_yn),
                        digit4 = test$digit4,
                        dfm.test.allword) %>%
  select(-doc_id)
dim(test.dfm.allword)

test.dfm.Noun <- data.frame(label=as.factor(test$seafood_yn),
                       digit4 = test$digit4, 
                       dfm.test.Noun) %>%
  select(-doc_id)

```

### preprocess test set using all words then convert it to 300 variable using svd
```{r}
#we already have document frequancy matrix above

### convert test data set to have the samee dimension as training data ##
dfm.test <- dfm_select(dfm.test, pattern = dfm.train)
dfm.test.matrix <- as.matrix(dfm.test)
dim(dfm.test)

########## Singular value decomposition ##########

test.svd <- t(sigma.inverse.2*u.transpose.2 %*% t(as.matrix(dfm.test)))

############### setup feature dataframe with label   ##########
test.svd <- data.frame(label=as.factor(test$seafood_yn), 
                       popseafood = test$popseafood, 
                       digit4 = test$digit4, 
                       test.svd)
dim(test.svd)

save(test.svd,file = "testsvd.Rdata")
load("testsvd.Rdata")

head(test.svd)

################# end of SVD ###################################
```

## model performance evaluation
```{r}
## create a function to determine best model
## we determine best model based on accuracy
## the fuction will return the best accuracy, tuning with the best accuracy,
## and confusion matrix for the model with the best accuracy
best.accuracy <- function(result, testdf) {
  accuracy <- 1:length(result)
  n.test <- nrow(testdf)
  for (i in seq_along(result)) {
    accuracy[[i]] <- sum(testdf$label == result[[i]] )/n.test 
  }
  CPR <- max(accuracy)
  k <- which.max(accuracy)
  conf.matrix <- confusionMatrix(as.factor(result[[k]]),testdf$label)
  best.model <- list(max_correct_predicted_rate =CPR,
                     best_thershold = k,
                     best_confusion.matric = conf.matrix)
  return(best.model)
}
```



## use KNN
KNN is a non-parameteric classification method where in order to predict a response class
we use the value of k-nearest neighbour in the data set. k indicate the number of neighbour we want to
use in the model. The distance between neighbour is determined using Euclidean Distance
```{r}
########################### knn #########################


##################### using simple test and training ##################

################ using dfm version of data ###########

# we can not use this type of data using KNN, because the alue between observation is 
# too much similar, there will be too many ties

########### ends of model using dfm version of data ##################


################ using svd version of data ###########
knn.result.svd <- vector(mode = "list",length=3)
tune.knn <- c(5,25,50,100,150) #low k might be overfit models

set.seed(1234)
for (i in seq_along(tune.knn)) {
  knn.result.svd[[i]] <- knn(train.svd[,-1],test.svd[,-1],train.svd$label, k = tune.knn[i])
}
save(knn.result.svd, file = "knn.result.svd.Rdata")
best.accuracy(knn.result.svd,testdf = test.svd)
system("say just finished")

test.predicted.actual.knnsvd <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = knn.result.svd[[1]]) ## best threshold is k = 5
test.predicted.actual.knnsvd %>%
  filter(actual != predicted)
########### ends of model using svd version of data ##################
```


## use logistic regression

```{r}
#using simple test and training, we previously use cross validation but ends up too long
############# using svd version of data #########
# time the code execution
start.time <- Sys.time()

logfit.svd <- glm(label ~ ., data = train.svd, family=binomial)

total.time.train.log.svd <- Sys.time() - start.time

## Took less than a minute to return a model
save(logfit.svd, file = "logfit.svd.Rdata")
total.time.train.log.svd

##################### predict with test ##################
# predict
logpredict <- predict(logfit.svd, newdata = test.svd,  type = "response")
log.result.svd <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  log.result.svd[[i]] <- 1*(logpredict > i/100 )
}

## determine best model
best.accuracy(log.result.svd, testdf = test.svd)
system("say just finished")


### examine the result 
library(broom)
coef.table.log.svd <- tidy(logfit.svd)
coef.table.log.svd
test.predicted.actual.logsvd <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = log.result.svd[[46]]) ## best threshold is 0.01
test.predicted.actual.logsvd %>%
  filter(actual != predicted)

############# using dfm with only Noun version of data #########
# time the code execution


logfit.noun <- glm(label ~ ., data = train.dfm.noun, family=binomial)
## there are multicollinerity problem in dataset
coef.table.log.noun <- tidy(logfit.noun)
coef.sig <- coef.table.log.noun %>%
  filter(p.value < 0.1)
term.sig <- paste(coef.sig$term[-1] ,collapse = "+")
logfit.noun <- glm(paste("label~", term.sig, sep = ""), data = train.dfm.noun, family=binomial)
save(logfit.noun, file = "logfit.noun.Rdata")
## Took less than a minute to return a model
# total.time.train.log.noun

##################### predict with test ##################
# predict
logpredict <- predict(logfit.noun, newdata = test.dfm.Noun,  type = "response")

log.result.noun <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  log.result.noun[[i]] <- 1*(logpredict > i/100 )
}

## determine best model
best.accuracy(log.result.noun, testdf = test.dfm.Noun)
system("say just finished")


test.predicted.actual.lognoun <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         prob = logpredict,
                         predicted = log.result.noun[[1]]) ## best threshold is 0.01 #
test.predicted.actual.lognoun %>%
  filter(actual != predicted)

############# using dfm with all words available to train of data with undersampling #########
# time the code execution
start.time <- Sys.time()

logfit.allword <- glm(label ~ ., data = train.dfm.under, family=binomial)
coef.table.log.allword <- tidy(logfit.allword)
coef.sig <- coef.table.log.allword %>%
  filter(p.value < 0.1)
term.sig <- paste(coef.sig$term[-1] ,collapse = "+")
logfit.allword <- glm(paste("label~", term.sig, sep = ""), data = train.dfm.under, family=binomial)
save(logfit.allword, file = "logfitallword.Rdata")
total.time.train.log.allword <- Sys.time() - start.time
## Took less than a minute to return a model
total.time.train.log.allword

##################### predict with test ##################
# predict
logpredict <- predict(logfit.allword, newdata = test.dfm.allword,  type = "response")
log.result.allword <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  log.result.allword[[i]] <- 1*(logpredict > i/100 )
}

## determine best model
best.accuracy(log.result.allword, testdf = test.dfm.allword)
system("say just finished")

coef.table.log.allword <- tidy(logfit.allword)
coef.table.log.allword %>%
  arrange(desc(estimate))
test.predicted.actual.logallword <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = log.result.allword[[27]]) ## best threshold is 0.33
test.predicted.actual.logallword %>%
  filter(actual != predicted)
```

## TREEE
```{r}
library(tree)
tree.1 <- tree( label ~ ., train.dfm.noun)
label.predict <-  predict( tree.1, test.dfm.Noun, type="class" )
confusionMatrix(label.predict,test.dfm.Noun$label)

##prune
cv <- cv.tree( tree.1, FUN = prune.misclass )
plot(cv)
which.min(cv$dev)

tree.1pruned <- prune.misclass( tree.1, best=24 )
label.predict.pruned <-  predict(tree.1pruned, test.dfm.Noun, type="class" )
confusionMatrix(label.predict.pruned,test.dfm.Noun$label)
save(tree.1, file="tree1.Rdata")

## all word
tree.2 <- tree( label ~ ., train.dfm.allword)
label.predict2 <-  predict( tree.2, test.dfm.allword, type="class" )

confusionMatrix(label.predict2,test.dfm.allword$label)
##prune
cv <- cv.tree( tree.2, FUN = prune.misclass )
plot(cv)
which.min(cv$dev)

tree.2pruned <- prune.misclass( tree.2, best=28 )
plot(tree.2pruned)
text(tree.2pruned)
label.predict.pruned2 <-  predict(tree.2pruned, test.dfm.allword, type="class" )
confusionMatrix(label.predict.pruned2,test.dfm.allword$label)
save(tree.2, file="tree2.Rdata")

#####  undersampling training
tree.3 <- tree( label ~ ., train.dfm.under)
label.predict3 <-  predict( tree.3, test.dfm.allword, type="class" )

confusionMatrix(label.predict3,test.dfm.allword$label)
save(tree.3, file="tree1.Rdata")
```


## SVM
```{r}
library(e1071)
svm <-  svm( label ~ ., data=train.dfm.noun, kernel="linear")
summary(svm)
labelpredictsvm <-predict(svm,test.dfm.Noun)
length(labelpredictsvm)
nrow(test.dfm)
confusionMatrix(labelpredictsvm,test.dfm.Noun$label)
save(svm,file = "svmundernoun.Rdata")

svm.2 <-  svm( label ~ ., data=train.dfm.under, kernel="linear")
labelpredictsvm2 <-predict(svm.2,test.dfm.allword)

confusionMatrix(labelpredictsvm2,test.dfm.allword$label)
save(svm.2,file = "svmunder.Rdata")

test.predicted.actual.svm2 <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = labelpredictsvm2) 
test.predicted.actual.svm2 %>%
  filter(actual != predicted)
```

#random forest
```{r}
library(randomForest)
library(doSNOW)

cl <- makeCluster(2)
registerDoSNOW(cl)

# time the code execution
start.time <- Sys.time()


rf <-  randomForest(label ~ ., data=train.dfm.under) 


total.time.rf <- Sys.time() - start.time

stopCluster(cl)

labelpredictrf.a <-predict(rf,test.dfm.allword)
confusionMatrix(labelpredictrf.a,test.dfm.allword$label)
save(rf,file = "rfunderCorrected.Rdata")
load("rfunder.Rdata")
rf
summary(rf)

labelpredictrf <-predict(rf,test.dfm.allword, type = "vote")

length(test.dfm.allword$label)
rf.result <- vector(mode = "list",length=100)

## generate list with different threshold
for (i in 1:100) {
  rf.result[[i]] <- 1*(labelpredictrf[,2] > i/100 )
}


best.accuracy(rf.result, testdf = test.dfm.allword)


test.predicted.actual.rf <- data.frame(name = test$name,
                         translate = test$translatedText,
                         actual = test$seafood_yn,
                         predicted = rf.result[[74]]) 
test.predicted.actual.rf %>%
  filter(actual != predicted)


```

## result
```{r}

#table
result <- rbind(
  data.frame(mode = "KNN", training = "decomposition",best.accuracy(knn.result.svd,testdf = test.svd)$best_confusion.matric$table), ## knn with variable decomposition
  data.frame(mode = "logistic reg", training = "decomposition", best.accuracy(log.result.svd, testdf = test.svd)$best_confusion.matric$table), ## logistic regressin with variable decomposition
  data.frame(mode = "logistic reg", training = "noun", best.accuracy(log.result.noun, testdf = test.dfm.Noun)$best_confusion.matric$table), ## logistic regressin with noun only
  data.frame(mode = "logistic reg", training = "under", best.accuracy(log.result.allword, testdf = test.dfm.allword)$best_confusion.matric$table), ## log reg with all words undersample
  data.frame(mode = "Decision tree", training = "noun", confusionMatrix(label.predict,test.dfm.Noun$label)$table),  ##decision tree with noun
  data.frame(mode = "Decision tree",training = "all words", confusionMatrix(label.predict2,test.dfm.allword$label)$table), ##decision tree with all word 
  data.frame(mode = "Decision tree", training = "under", confusionMatrix(label.predict3,test.dfm.allword$label)$table),  ##decision tree with all word undersample
  data.frame(mode = "svm", training = "noun", confusionMatrix(labelpredictsvm,test.dfm.Noun$label)$table),  ##svm with only noun
  data.frame(mode = "svm", training = "under", confusionMatrix(labelpredictsvm2,test.dfm.allword$label)$table), ##svm with allword undersample
  data.frame(mode = "random forest", training = "under", best.accuracy(rf.result, testdf = test.dfm.allword)$best_confusion.matric$table)
)#random forest with all word undersample


result %>%
  group_by(mode, training, Reference) %>%
  mutate(sumreference = ifelse(Reference == 0, sum(Freq[Reference == 0]),sum(Freq[Reference == 1]) )) %>%
  mutate(rate = Freq/sumreference) %>%
  arrange(mode,training, Prediction)

result%>%
  filter(Prediction != Reference) %>%
  ggplot(aes(training,Freq, fill= Prediction)) + geom_col() + facet_wrap(~mode)

result%>%
  filter(Prediction != Reference) %>%
  ggplot(aes(Reference, Freq, fill= Prediction)) + geom_col() + facet_grid(mode~training) + theme_bw()
  
```

```{r}
unlabeled <- read_csv("dfunlabeledtranslated.csv")

test.unlabeled <- sample_n(unlabeled, 1000, seed = 1)
head(test.unlabeled)

test.unlabeled <- test.unlabeled %>%
  mutate(digit4 = ifelse(str_detect(translatedText,"[:digit:]{4}"),1,0))
## preprocessing unlabeled
nrow(test.unlabeled)
#create a corpus
corpus <- corpus(test.unlabeled$translatedText) 

#create document frequency matrix with verb
dfm.unlabeled <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)

dim(dfm.unlabeled)

dfm.unlabeled <- dfm_select(dfm.unlabeled, pattern = dfm.train.under)

dfm.unlabeled <- data.frame(label=as.factor(0),
                            digit4 = test.unlabeled$digit4,
                        dfm.unlabeled) %>%
  select(-doc_id)

dim(dfm.unlabeled)
labelpredict.unlabeled <-predict(rf,dfm.unlabeled, type = "vote")
rf.result <- vector(mode = "list",length=100)
rf.result <- 1*(labelpredict.unlabeled[,2] > 74/100 )
test.unlabeled$predicted <- rf.result
test.unlabeled%>%
  select(translatedText, predicted) %>%
  write_csv(file = "predictedunlabeled.csv")
table(test.unlabeled$predicted)
unlabeled.predicted <- read_csv("predictedunlabeled.csv")
confusionMatrix(as.factor(unlabeled.predicted$predicted), as.factor(unlabeled.predicted$actual.manual))
unlabeled.predicted %>%
  filter( predicted != actual.manual)

```

## predict all
```{r}
load("rfunderCorrected.Rdata")
unlabeled <- read_csv("dfunlabeledtranslated.csv")

unlabeled <- unlabeled %>%
  mutate(digit4 = ifelse(str_detect(translatedText,"[:digit:]{4}"),1,0))
## preprocessing unlabeled
#create a corpus
corpus <- corpus(unlabeled$translatedText) 

#create document frequency matrix with verb
dfm.unlabeled <- dfm(corpus, 
             tolower=TRUE,  
             remove_punct =TRUE, 
             remove_numbers=TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove=stopwords('english', source = "snowball"), 
             stem=TRUE, 
             verbose=TRUE)

dfm.unlabeled <- dfm_select(dfm.unlabeled, pattern = dfm.train.under)



dim(dfm.unlabeled)
labelpredict.unlabeled <- 1:nrow(dfm.unlabeled)
length(labelpredict.unlabeled)

## using loop because wa can't do all in one (memory exhaust)
for (i in 1:2000) {
  data <- data.frame(label=as.factor(0),
                            digit4 = unlabeled$digit4[[i]],
                        dfm.unlabeled[i,]) %>%
  select(-doc_id)
  labelpredict <-predict(rf,data, type = "vote")
  rf.result <- 1*(labelpredict[,2] > 74/100 )
  labelpredict.unlabeled[[i]] <- rf.result
  print(i)
  }


labelpredict.unlabeled <- labelpredict.unlabeled[1:2000]
unlabeled$seafood_yn[1:2000] <- labelpredict.unlabeled
df.year <- unlabeled[1:2000,] %>%
  select(times_appeared, first_appeared, last_appeared, seafood_yn, translatedText)

df.year <- df.year %>%
  filter(times_appeared>0)
data <- vector(mode = "list", length = nrow(df.year))
for (i in seq_along(data)) {
  name.vec <- rep(df.year$translatedText[i], df.year$times_appeared[i])
year.vec <- if (df.year$times_appeared[i] == 1) {
  c(df.year$first_appeared[i])
  } else if (df.year$times_appeared[i] == 2) {
  c(df.year$first_appeared[i], df.year$last_appeared[i])
    } else {
  c(df.year$first_appeared[i], df.year$last_appeared[i],
    sample(df.year$first_appeared[i]:df.year$last_appeared[i], df.year$times_appeared[i]-2, replace = TRUE)
  )
    } 
data[[i]] <- data.frame (translatedText = name.vec,
                    year = year.vec)
}

data <- do.call(rbind, data)

data <- data %>%
  left_join(df.year)

data %>%
  filter(seafood_yn == 1) %>%
  mutate(oyster = str_detect(translatedText, "oyster|Oyster")*1) %>%
  group_by(year) %>%
  summarise(n = sum(seafood_yn), 
            oyster = sum(oyster)) %>%
  filter(year > 1900) %>%
  gather(n:oyster, key= key, value = val) %>%
  ggplot(aes(year, val, color=key)) + geom_line() + theme_classic()

              

```
## visual
```{r}
unlabeled.predicted
```

